#!/usr/bin/env python3

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
from taxfinder import TaxFinder
from collections import defaultdict

from phylogenetics import phylogenetics as phylo


class ConfigReader():
	'''
	The ConfigReader reads the `limits.txt`, `proteinlist.txt`, `heatmap_config.txt`, and `crosshits.txt` and returns the contents in different formats upon request.
	'''

	def __init__(self):
		'''
		Initiates the class.
		'''

		self.limits_dict = {'default': (1e-30, 50)}
		self.proteins = []
		self.protein_files = []
		self.taxa_to_show = []
		self.hm_colors = {}
		self.clustering_method = ''
		self.crosshits_to_analyse = []
		self.crosscrosshits = []

		self._read_proteinlist()
		self._read_limits()
		self._read_heatmap_values()


	def _read_limits(self):
		'''
		Reads `limits.txt` and saves the content as dictionary to be used by the `get_limits()` method.
		'''

		with open('limits.txt') as f:
			self.limits_dict = {'default': (1e-30, 50)}
			for line in f:
				line = line.split('#')[0].rstrip()
				if not line:
					continue
				lline = line.split()
				try:
					evalue = float(lline[1])
				except ValueError:
					evalue = self.limits_dict['default'][0]
				try:
					length = int(lline[2])
				except ValueError:
					length = self.limits_dict['default'][1]
				self.limits_dict[lline[0]] = (evalue, length)


	def get_limits(self):
		'''
		Reads `limits.txt` and returns a dict in the format: {'protein': (evalue, length), ...} where evalue is the negative exponent of the evalue (int) and length is the length limit for the protein (int).

		:uses: `limits.txt`
		:returns: A dictionary with limits as described above
		'''

		return self.limits_dict


	def _read_proteinlist(self):
		'''
		Reads `proteinlist.txt` and saves the content as lists to be used by the get* methods.
		'''

		with open('proteinlist.txt') as f:
			self.proteins = []
			self.protein_files = []
			for line in f:
				line = line.split('#')[0].strip()
				if not line:
					continue
				elems = line.split()
				if elems[0] not in self.proteins:
					self.proteins.append(elems[0])
					self.protein_files.append([])
				pidx = self.proteins.index(elems[0])
				self.protein_files[pidx].append(os.path.splitext(elems[1])[0])


	def get_protein_names(self, prefix = '', suffix = ''):
		'''
		Returns a set of proteins in the format: {'protein1', 'protein2', ...}. A prefix (a path) can be added as well as a suffix. get_protein_names('abc/', '.txt') will turn 'protein1' into 'abc/protein1.txt'.

		:param prefix: A string to be prepended before the name
		:param suffix: A string to be appended after the name
		:returns: A set with proteins
		'''

		return set((prefix + protein + suffix for protein in self.proteins))


	def get_protein_files(self, prefix = '', suffix = ''):
		'''
		Returns a set of filenames in the format: {'proteinfile1', 'proteinfile2', ...}. The files are just the basenames without suffix. A prefix (a path) can be added as well as a suffix. get_protein_files('abc/', '.txt') will turn 'file1' into 'abc/file1.txt'.

		:param prefix: A string to be prepended before the filename
		:param suffix: A string to be appended after the filename
		:returns: A set of filenames
		'''

		return set((prefix + fn + suffix for proteinlist in self.protein_files for fn in proteinlist))


	def get_protein_dict(self, prefix = '', suffix = ''):
		'''
		Returns a dict with proteins and their respective file names in the format: {'protein': set('file1', 'file2', ...), ...}. The files are just the basenames without suffix. A prefix (a path) can be added as well as a suffix. get_protein_dict('abc/', '.txt') will turn 'file1' into 'abc/file1.txt'.

		:param prefix: A string to be prepended before the filename
		:param suffix: A string to be appended after the filename
		:returns: A dictionary as described above
		'''

		ret = {}
		for i in range(len(self.proteins)):
			ret[self.proteins[i]] = set((prefix + fn + suffix for fn in self.protein_files[i]))

		return ret


	def _read_heatmap_values(self):
		'''
		Reads `heatmap_config.txt` and saves the content as lists and dicts to be used by the get* methods.
		'''

		with open('heatmap_config.txt') as f:
			self.taxa_to_show = []
			self.hm_colors = {}
			self.clustering_method = ''

			mode = ''
			modes = {'TAXA_TO_SHOW': 'taxa_to_show', 'COLORS': 'colors', 'ALGO': 'clustering'}

			clustering_methods = {'single', 'complete', 'average', 'weighted', 'centroid', 'median', 'ward'}

			for line in f:
				line = line.rstrip()
				if not line or line.startswith('#'):
					continue

				if line in modes:
					mode = modes[line]

				elif mode == 'taxa_to_show':
					self.taxa_to_show.append(line)

				elif mode == 'colors':
					lline = line.split()
					self.hm_colors[lline[0]] = lline[1]

				elif mode == 'clustering':
					if line in clustering_methods:
						self.clustering_method = line
					else:
						raise ValueError('Error in heatmap_config.txt. Unknown clustering method: {}!'.format(line))

				else:
					raise ValueError('Error in heatmap_config.txt. No mode selected and {} found!'.format(line))


	def get_heatmap_taxa(self):
		'''
		Returns a list with the taxa to show.

		:returns: A list of taxa
		'''

		return self.taxa_to_show


	def get_heatmap_colors(self):
		'''
		Returns a dict with the colors in the form {'letter': 'color_code', ...}

		:returns: A dict with colors
		'''

		return self.hm_colors


	def get_heatmap_clustering(self):
		'''
		Returns the clustering method of choice

		:returns: A string with the clustering method
		'''

		return self.clustering_method


	def _read_crosshits(self):
		'''
		Reads `crosshits.txt` and saves the content as list(s) to be used by the get* methods.
		'''

		with open('crosshits.txt') as f:
			self.crosshits_to_analyse = []
			for line in f:
				line = line.split('#')[0].rstrip()
				if not line:
					continue

				if line.startswith('!crosscrosshits'):
					if line == '!crosscrosshits':
						self.crosscrosshits = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50]
					else:
						self.crosscrosshits = sorted([int(x) for x in line.split()[1:]])
					continue

				self.crosshits_to_analyse.append(line)


	def get_crosshits_to_analyse(self):
		'''
		Returns a list with the protein names to use for the crosshit analysis.

		:returns: A list of protein names
		'''

		return self.crosshits_to_analyse


	def get_crosscrosshits(self):
		'''
		Returns a list with the clustering borders (ints) for crosscrosshits.

		:returns: A list of clustering borders (ints)
		'''

		return self.crosscrosshits

	# end ConfigReader


def _my_get_basename(name):
	'''
	Strips the path and the extension from a filename. E.g. /a/path/to/file.png -> file

	:param name: The file and path to be processed
	:returns: The processed filename
	'''

	return os.path.splitext(os.path.basename(name))[0]


def init():
	'''
	Creates some files and a folder to start with a new project. Existing files will not be overwritten.

	:creates: .gitignore
	:creates: limits.txt
	:creates: proteinlist.txt
	:creates: tree_config.txt
	:creates: tree_to_prune.txt
	:creates: crosshits.txt
	:creates: heatmap_config.txt
	:creates: heatmap_template.html
	:creates: fastas/
	:creates: blastresults/
	'''

	os.makedirs('fastas', exist_ok=True)
	os.makedirs('blastresults', exist_ok=True) # In case the user runs Blast separately

	orig_dir = os.path.dirname(os.path.realpath(__file__))

	if not os.path.isfile('.gitignore'):
		with open('.gitignore', 'w') as out, open(os.path.join(orig_dir, 'templates/gitignore')) as f:
			out.write(f.read())

	if not os.path.isfile('limits.txt'):
		with open('limits.txt', 'w') as out, open(os.path.join(orig_dir, 'templates/limits.txt')) as f:
			out.write(f.read())

	if not os.path.isfile('proteinlist.txt'):
		with open('proteinlist.txt', 'w') as out, open(os.path.join(orig_dir, 'templates/proteinlist.txt')) as f:
			out.write(f.read())

	if not os.path.isfile('tree_config.txt'):
		with open('tree_config.txt', 'w') as out, open(os.path.join(orig_dir, 'templates/tree_config.txt')) as f:
			out.write(f.read())

	if not os.path.isfile('tree_to_prune.txt'):
		with open('tree_to_prune.txt', 'w') as out, open(os.path.join(orig_dir, 'templates/tree_to_prune.txt')) as f:
			out.write(f.read())

	if not os.path.isfile('crosshits.txt'):
		with open('crosshits.txt', 'w') as out, open(os.path.join(orig_dir, 'templates/crosshits.txt')) as f:
			out.write(f.read())

	if not os.path.isfile('heatmap_config.txt'):
		with open('heatmap_config.txt', 'w') as out, open(os.path.join(orig_dir, 'templates/heatmap_config.txt')) as f:
			out.write(f.read())

	if not os.path.isfile('heatmap_template.html'):
		with open('heatmap_template.html', 'w') as out, open(os.path.join(orig_dir, 'templates/heatmap_template.html')) as f:
			out.write(f.read())


def run_blast(db):
	'''
	Blasts a list of files with Blastp against the provided database.

	:param db: The database to blast against
	:creates: `blastresults/*.xml`
	'''

	os.makedirs('blastresults', exist_ok=True)

	if not db:
		raise ValueError('blastdb is empty. Run this script with -d /path/to/blastdb')

	file_list = CR.get_protein_files(prefix = 'fastas/', suffix = '.fasta')
	outstring = 'Now blasting {:' + str(len(str(len(li)))) + 'd}/{}: {}'

	for i, filename in enumerate(file_list):
		print(outstring.format(i+1, len(file_list), filename))
		outfilename = 'blastresults/{}.xml'.format(_my_get_basename(filename))
		phylo.run_blastp(filename, outfilename, db)


def parse_blast_results(to_parse = None):
	'''
	Parses Blast results to a table (tsv file).

	:param to_parse: Should be an iterable with filenames that shall be parsed. If all files in `blastresults` shall be parsed, this must be None.
	:creates: `resulttables/*.tsv`
	'''

	if to_parse is None:
		to_parse = CR.get_protein_files(prefix = 'blastresults/', suffix = '.xml')

	os.makedirs('resulttables', exist_ok=True)

	outstring = 'Now parsing {:' + str(len(str(len(li)))) + 'd}/{}: {:<30}'

	for i, filename in enumerate(to_parse):
		basename = _my_get_basename(filename)
		print(outstring.format(i+1, len(to_parse), basename), end='\r')
		parsed_result = phylo.parse_blast_result(filename, TF = TF, top = 0, exclude={118797, 59538, 7213})
		# Contaminants:
		#    118797: Lipotes vexillifer
		#     59538: Pantholops hodgsonii
		#      7213: Ceratitis capitata

		open('resulttables/{}.tsv'.format(basename), 'w').write(parsed_result)


def combine_parsed_results():
	'''
	Combines parsed Blast results according on proteinlist.txt.

	:creates: `combinedtables/*.tsv`
	'''

	os.makedirs('combinedtables', exist_ok=True)

	limits = CR.get_limits()
	proteins = CR.get_protein_dict(prefix = 'resulttables/', suffix = '.tsv')

	for k in sorted(proteins):
		print('combining tsv files... {:<40}'.format(k), end='\r')
		outfn = 'combinedtables/{}.tsv'.format(k)
		try:
			max_evalue, min_length = limits[k]
		except KeyError:
			max_evalue, min_length = limits['default']
		header = True

		combined = phylo.combine_parsed_results(proteins[k], max_evalue, min_length)

		if combined:
			open(outfn, 'w').write(combined)


def tables_for_interactive_heatmap():
	'''
	Combines parsed Blast results for use for interactive heatmaps.

	:creates: `interactivetables/*.tsv`
	'''

	proteins = CR.get_protein_dict(prefix = 'resulttables/', suffix = '.tsv')

	os.makedirs('interactivetables', exist_ok=True)

	for k in sorted(proteins):
		print(k.ljust(50), end='\r')

		entries = phylo.table_for_interactive_heatmaps(proteins[k], TF)

		with open('interactivetables/{}.tsv'.format(k), 'w') as outfile:
			for m in sorted(entries):
				outfile.write('{}\t{}\n'.format(m, entries[m]))


def unique_names():
	'''
	For each protein, creates lists with the species that possess this protein. Two lists are generated: One with unique species names and one with unique taxonomy ids. In addition a general.names and general.taxids are generated with all species that occur in the lists of all proteins.

	:creates: `names/*.names`
	:creates: `taxids/*.taxids`
	'''

	os.makedirs('names', exist_ok=True)
	os.makedirs('taxids', exist_ok=True)

	total_names = set()
	total_taxids = set()

	for fn in CR.get_protein_names():
		names, taxids = phylo.unique_names_and_taxids('combinedtables/{}.tsv'.format(fn))

		with open('names/{}.names'.format(fn), 'w') as f:
			for name in names:
				f.write(name + '\n')

		with open('taxids/{}.taxids'.format(fn), 'w') as f:
			for taxid in taxids:
				f.write(str(taxid) + '\n')

		total_names.update(names)
		total_taxids.update(taxids)

	with open('names/general.names', 'w') as f:
		for name in sorted(total_names):
			f.write(name + '\n')

	with open('taxids/general.taxids', 'w') as f:
		for taxid in sorted(total_taxids):
			f.write(str(taxid) + '\n')


def make_newick():
	'''
	Creates Newick trees for every taxonomy id list in the taxids folder.

	:creates: `trees/*.tre`
	'''

	todo = CR.get_protein_names(prefix = 'taxids/', suffix = '.taxids')
	todo.add('taxids/general.taxids')

	os.makedirs('trees', exist_ok=True)

	sanitizer = phylo.NodeSanitizer()

	for filename in todo:
		outfn = 'trees/{}.tre'.format(_my_get_basename(filename))
		newick = phylo.make_newick(filename, sanitizer, TF)

		open(outfn, 'w').write(newick)

	sanitizer.print_bad_chars()


def tree_attributes():
	'''
	For each element in the general tree, determine the proteins (attributes) for this element. Each attribute will be a two-letter string. The meaning of each string will be written to `attributekeys.txt`.

	:uses: `trees/general.tre`
	:creates: `attributekeys.txt` (containing an explanation of the keys)
	:creates: `attributes.txt` (containing the keys for each protein)
	'''

	proteinlist = sorted(CR.get_protein_names())
	filenames = {name: 'trees/{}.tre'.format(name) for name in proteinlist}

	master_tree = open('trees/general.tre').read()

	keys, attributes = phylo.get_keys_and_attributes(proteinlist, filenames, master_tree)

	with open('attributekeys.txt', 'w') as out:
		for key in sorted(keys):
			out.write('{} -> {}\n'.format(key, keys[key]))

	with open('attributes.txt', 'w') as out:
		for k in sorted(attributes):
			out.write('{}\t{}\n'.format(k, ''.join(attributes[k])))


def make_histograms():
	'''
	For each seed protein, the distribution of hits is shown in a histogram.

	:creates: `histograms/*.png`
	'''

	protein_files = CR.get_protein_dict(prefix = 'fastas/', suffix = '.fasta')
	seed_lengths = {}

	for prot in protein_files:
		seed_lengths[prot] = []
		for f in protein_files[prot]:
			with open(f) as fastafile:
				length = 0
				next(fastafile)
				for line in fastafile:
					length += len(line.rstrip())
			seed_lengths[prot].append(length)

	os.makedirs('histograms', exist_ok=True)

	cm = plt.cm.get_cmap('rainbow')

	for protein in sorted(protein_files):
		fn = 'combinedtables/{}.tsv'.format(protein)
		print('Histogram: {:<50}'.format(protein), end='\r')

		fig = phylo.make_histogram(fn, seed_lengths[protein], colormap = cm)

		fig.savefig('histograms/{}.png'.format(protein))


def show_blast_mapping():
	'''
	For each protein, create an overview over where the hits where mapped over the length of the protein.

	:creates: `blastmappings/*.png`
	'''

	os.makedirs('blastmappings', exist_ok=True)

	filenames = sorted(CR.get_protein_files())

	for filename in filenames:
		print('Mapping {:<50}'.format(filename), end='\r')

		query_length = 0
		with open('fastas/{}.fasta'.format(filename)) as f:
			next(f)
			for line in f:
				query_length += len(line.rstrip())

		filename = 'blastresults/{}.xml'.format(filename)

		im = phylo.show_blast_mapping(filename, query_length)

		#im.show()
		im.save('blastmappings/{}.png'.format(filename))


def similarity_matrix():
	'''
	Creates a similarity matrix of proteins. For each protein, the Blast hits are compared to each other protein. The lowest e-value of each protein-protein combination is saved. This gives an impression of how similar two given proteins are and how much the results overlap.

	:uses: combined parsed Blast results in `combinedtables`
	:creates: `matrix.csv`
	'''

	names = {name: 'combinedtables/{}.tsv'.format(name) for name in CR.get_protein_names()}

	res = phylo.similarity_matrix(names)

	with open('matrix.csv', 'w') as out:
		out.write('\t' + '\t'.join(sorted(names)) + '\n')
		for i, line in enumerate(res):
			out.write(names[i] + '\t' + '\t'.join(map(str, line)) + '\n')


def int_heatmap():
	'''
	Creates an interactive heatmap as html file with javascript.

	:uses: heatmap_template.html
	:creates: out.html
	'''

	taxa_to_check = CR.get_heatmap_taxa()
	colors = CR.get_heatmap_colors()
	method = CR.get_heatmap_clustering()
	proteins_to_check = sorted(CR.get_protein_names())
	template = open('heatmap_template.html').read()

	taxids = {}
	tick_taxons = []
	for i, tax in enumerate(taxa_to_check):
		taxname, taxid = tax.split('^')
		tick_taxons.append(taxname.replace('_', ' '))
		taxids[taxid] = i

	tick_proteins = proteins_to_check[:]

	matrix = []
	for i, protein in enumerate(proteins_to_check):
		matrix.append([-100] * len(taxa_to_check))
		with open('interactivetables/{}.tsv'.format(protein)) as f:
			for line in f:
				lline = line.split()
				if lline[0] in taxids:
					matrix[i][taxids[lline[0]]] = int(lline[1])

	html = phylo.interactive_heatmap(matrix, tick_taxa, tick_proteins, colors, template, method)

	open('out.html', 'w').write(html)


### not moved to the module, yet.
def get_crosshits(to_analyse = None, crosscrosshits = None):
	'''
	For each pair of proteins creates a file '{protein1}-{protein2}-crosshits.tsv'
	File has three columns: e-value, number of crosshits, AccID^TaxID for all crosshits (comma-separated)

	:param to_analyse: Should be a list with proteinnames that shall be used. If it evaluates to False, proteins in `crosshits.txt` will be used instead.
	:param crosscrosshits: A list with clustering borders for crosscrosshits. If not given, it will be tried to be read from `crosshits.txt`. To disable crosscrosshitting, omit or give None.
	:creates: `crosshits/*.tsv`
	'''

	if to_analyse is None:
		to_analyse = CR.get_crosshits_to_analyse()

	if crosscrosshits is None:
		crosscrosshits = CR.get_crosscrosshits()

	if not to_analyse:
		return

	os.makedirs('crosshits', exist_ok=True)

	values = {}

	if crosscrosshits:
		# set is important! the combined tables don't have unique entries
		taxids = defaultdict(set)

	for name in to_analyse:
		values[name] = [set() for _ in range(151)]
		with open('combinedtables/{}.tsv'.format(name)) as f:
			next(f)
			for line in f:
				lline = line.split('\t')
				evalue = lline[4]
				if 'e-' in evalue:
					evalue = int(evalue.split('e-')[1])
					if evalue > 150:
						evalue = 150
				elif evalue == '0.0':
					evalue = 150
				else:
					evalue = 0
				acc = lline[1]
				taxid = lline[0]
				# Adding TaxID for later use, Acc-IDs are already unique (and implicate tax-id) so this should change anything
				values[name][evalue].add(acc+'^'+taxid)
				if crosscrosshits:
					taxids[taxid].add(acc)

	names = sorted(values)

	for i, name1 in enumerate(names):
		for name2 in names[i:]: # skips symmetric combinations
			if name1 == name2:  # don't need to compare with itself
				continue
			acc1 = set()
			acc2 = set()
			common = [set() for _ in range(151)]
			oldhits = set()
			for evalue in range(150, -1, -1):
				acc1.update(values[name1][evalue])
				acc2.update(values[name2][evalue])
				inter = acc1.intersection(acc2)
				common[evalue] = inter - oldhits # only write those hits that are new for this evalue
				oldhits.update(inter)

			with open('crosshits/{}-{}.tsv'.format(name1, name2), 'w') as out:
				out.write('#e-value\tnumber-of-crosshits\tAccID^TaxID comma-sep\n')
				for j, line in enumerate(common):
					out.write('{}\t{}\t{}\n'.format(j, len(line), ','.join(list(line))))

			if crosscrosshits:
				last = []
				cchits = defaultdict(list)
				for entry in oldhits:
					tax = entry.split('^')[1]
					for n in crosscrosshits:
						if len(taxids[tax]) <= n:
							cchits[n].append(tax)
							break
					else:
						last = [crosscrosshits[-1]+1]
						cchits[crosscrosshits[-1]+1].append(tax)

				with open('crosshits/{}-{}-crosscrosshits.csv'.format(name1, name2), 'w') as out:
					out.write('#This file contains clusters of crosscrosshits\n')
					for n in crosscrosshits + last:
						if n in last:
							out.write('!>{} crosshits per species\n'.format(n-1))
						elif n-1 and n-1 not in crosscrosshits:
							if crosscrosshits.index(n):
								lower = crosscrosshits[crosscrosshits.index(n) - 1] + 1
							else:
								lower = 1
							out.write('!{} - {} crosshits per species\n'.format(lower, n))
						else:
							out.write('!{} crosshits per species\n'.format(n))

						out.write(','.join(cchits[n])+'\n')


### not moved to the module, yet.
def cross_histograms():
	'''
	### NOT DESCRIPTION YET!!!
	'''

	combos = [f[:-4] for f in os.listdir('crosshits') if f.endswith('.tsv')]
	data = {}

	for c in combos:
		with open('crosshits/{}.tsv'.format(c)) as f:
			next(f)
			lowest = 0
			d = np.array([range(151), [0]*151], dtype=np.int)
			for line in f:
				ev, num, accs = line.rstrip('\n').split('\t')
				if not lowest and int(num):
					lowest = int(ev)
				d[1][int(ev)] = int(num)
			try:
				highest = max([ev for ev, n in enumerate(d[1]) if n]) # highest ev with non-0 crosshits
			except ValueError:
				highest = 0
			data[c] = (lowest, highest, d)

	for combo in combos:
		print('Histogram: {:<50}'.format(combo), end='\r')
		values = data[combo][2]

		mi = data[combo][0] #(0) 30 or higher
		ma = data[combo][1] #max 150
		if len(values[1][mi:ma]) > 0:
			mean = int(np.mean(values[1][mi:ma]))
			median = int(np.median(values[1][mi:ma]))
		else:
			mean = '-'
			median = '-'

		text = '''Distribution
    min: {}
    max: {}
    average: {}
    median: {}'''.format(mi, ma, mean, median)

		fig = plt.figure(1, figsize=(12, 6))
		ax = fig.add_subplot(1, 1, 1)
		bars = ax.bar(values[0], values[1], width=1)

		ax.text(0.05, 0.95, text, transform=ax.transAxes, horizontalalignment='left', verticalalignment='top')
		ax.set_xlabel('E-value [10^-X]')
		ax.set_ylabel('Number of crosshits')

		fig.savefig('crosshits/{}-distribution.png'.format(combo))

		fig.clear()


tasknames = ['blast', 'parse', 'combine', 'comheat', 'unique', 'newick', 'attrib', 'hist', 'map', 'intheat', 'matrix', 'crosshits', 'crosshist']

tasks = {'blast': ('run Blast', run_blast),
'parse': ('parse Blast results', parse_blast_results),
'combine': ('combine parsed results', combine_parsed_results),
'comheat': ('combine parsed results for heatmaps', tables_for_interactive_heatmap),
'unique': ('create unique lists of names and taxids', unique_names),
'newick': ('create Newick tree for each protein', make_newick),
'attrib': ('determine tree attributes', tree_attributes),
'hist': ('create histograms with Blast hits for each protein', make_histograms),
'map': ('create hit mapping diagrams for each protein', show_blast_mapping),
'intheat': ('create an interactive heatmap (html)', int_heatmap),
'matrix': ('create a similarity matrix of all proteins', similarity_matrix),
'crosshits': ('create files with all blast crosshits of certain proteins', get_crosshits),
'crosshist': ('creates Histograms of e-value distribution for crosshits', cross_histograms)}


def run_workflow(start, end=''):
	'''
	Starts the workflow from `start` until `end`. If `end` is empty, the workflow is run until the last element of the workflow.

	:param start: The name of the first element to run.
	:param end: The name of the last element to run or a falsy value if the workflow shall be run until the end.
	'''

	if start not in tasknames:
		raise ValueError('{} is no valid task.'.format(start))

	if not end:
		endidx = len(tasknames)
	elif end not in tasknames:
		raise ValueError('{} is no valid task.'.format(end))
	else:
		endidx = tasknames.index(end) + 1

	startidx = tasknames.index(start)
	for taskname in tasknames[startidx:endidx]:
		print('{}: "{}"'.format(taskname, tasks[taskname][0]))
		task = tasks[taskname][1]
		task()


if __name__ == '__main__':
	import argparse
	import textwrap

	workflow = '\n'.join(textwrap.wrap('''The following is a list of the workflow. The names or numbers can be used for the -s or -o arguments.''', width = 80))

	workflow += '\n\n' + '\n'.join(('{:>2}. {:<8} {}'.format(i, name, tasks[name][0]) for i, name in enumerate(tasknames)))

	parser = argparse.ArgumentParser(description='This module provides you with tools to run phylogenetic analyses. Exactly one argument must be given.')

	parser.add_argument('-l', '--list', action='store_true', help='Shows the whole workflow with information and exits')
	parser.add_argument('-i', '--init', action='store_true', help='Initiates the working directory with necessary files and folders')
	parser.add_argument('-a', '--all', action='store_true', help='Run the full workflow without Blast')
	parser.add_argument('-b', '--blast', action='store_true', help='Run the full workflow including Blast')
	parser.add_argument('-s', '--startfrom', default='', help='Run from and including this step [e.g. 7 or hist]')
	parser.add_argument('-o', '--only', default='', help='Run only the given step [e.g. 4 or unique]')
	parser.add_argument('-d', '--database', default='', help='Path to the Blast database to use. Only needed when actually running Blast (-b or -[os] blast)')

	args = parser.parse_args()

	if args.list:
		parser.print_help()
		print('')
		print(workflow)
		sys.exit()

	if args.init:
		init()
		sys.exit()

	num_arguments = args.all + args.blast + bool(args.startfrom) + bool(args.only)

	if not (num_arguments == 1 or (num_arguments == 2 and args.database != '')):
		parser.print_help()
		sys.exit()

	blastdb = args.database

	# We need objects of these two classes for most of the functions, so we initialize them here already
	# TaxFinder takes some seconds to load, so this is, what makes loading this module slow.
	TF = TaxFinder()
	try:
		CR = ConfigReader()
	except IOError:
		CR = None

	if args.all:
		run_workflow('parse')
	elif args.blast:
		run_workflow('blast')
	elif args.startfrom:
		try:
			a = int(args.startfrom)
		except ValueError:
			try:
				a = tasknames.index(args.startfrom)
			except ValueError:
				parser.print_help()
		if a < len(tasknames):
			run_workflow(tasknames[a])
		else:
			parser.print_help()
	elif args.only:
		try:
			a = int(args.only)
		except ValueError:
			try:
				a = tasknames.index(args.only)
			except ValueError:
				parser.print_help()
		if a < len(tasknames):
			task = tasks[tasknames[a]][1]
			task()
		else:
			parser.print_help()
	else:
		print('This should not happen!')
		parser.print_help()
else:
	# We need objects of these two classes for most of the functions, so we initialize them here already
	# TaxFinder takes some seconds to load, so this is, what makes loading this module slow.
	TF = TaxFinder()
	try:
		CR = ConfigReader()
	except IOError:
		CR = None

	blastdb = ''
